{"cells":[{"cell_type":"markdown","source":["###### Description: In this notebook we read landlord state rows from incoming csv files into a streamig dataframe, transform (clean, cast, rename) the data, add/update the latest state to a static hive tabe\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\" --> \"landlord_data\""],"metadata":{}},{"cell_type":"code","source":["import requests\nimport json\nimport optimus as op\nimport phonenumbers \nimport re\nimport datetime\n\nfrom pyspark.sql.types import StringType, IntegerType, TimestampType, DateType, DoubleType, StructType, StructField\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window as W\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import rank, col\nimport time"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Schema for Landlord JSON\nlandlord_schema = StructType([\n            StructField(\"Landlord_id\", IntegerType(), False),\n            StructField(\"Password\", StringType(), True),\n            StructField(\"Landlord_name\", StringType(), False),\n            StructField(\"Address_line_1\", StringType(), False),\n            StructField(\"City\", StringType(), False),\n            StructField(\"Post_code\", StringType(), True),\n            StructField(\"Region\", StringType(), True),\n            StructField(\"event_time\", TimestampType(), True),\n            StructField(\"fetch_time\", StringType(), True)])\n\n# Schema for building JSON\nbuilding_schema = StructType([\n            StructField(\"Building_name\", StringType(), True),\n            StructField(\"Landlord_id\", IntegerType(), False),\n            StructField(\"Address_line_1\", StringType(), False),\n            StructField(\"City\", StringType(), False),\n            StructField(\"Post_code\", StringType(), True),\n            StructField(\"Region\", StringType(), True)])\n\n# Schema for Apartment JSON\napartment_schema = StructType([\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Type\", StringType(), True),\n            StructField(\"Rent_fee\", StringType(), True),\n            StructField(\"Building_name\", StringType(), True),\n            StructField(\"Appt_details\", StringType(), True)])\n\n# Schema for Contractor\ncontractor_schema = StructType([\n            StructField(\"Contract_id\", IntegerType(), False),\n            StructField(\"Name\", StringType(), True),\n            StructField(\"Address_line_1\", StringType(), False),\n            StructField(\"City\", StringType(), False),\n            StructField(\"Post_code\", StringType(), True),\n            StructField(\"Region\", StringType(), True)])\n\n# Schema for Tenant\ntenant_schema = StructType([\n            StructField(\"Tenant_id\", IntegerType(), False),\n            StructField(\"First_name\", StringType(), True),\n            StructField(\"Last_name\", StringType(), False),\n            StructField(\"Ssn\", StringType(), True),\n            StructField(\"Phone\", StringType(), True),\n            StructField(\"Email\", StringType(), True), \n            StructField(\"Mobile\", StringType(), True)])\n\n# Schema for Lease \nlease_schema = StructType([\n            StructField(\"Lease_id\", IntegerType(), False),\n            StructField(\"Start\", StringType(), True),\n            StructField(\"End\", StringType(), False),\n            StructField(\"Deposit\", StringType(), True),\n            StructField(\"Tenant_id\", IntegerType(), True),\n            StructField(\"Apartment_id\", IntegerType(), True)])\n\n# Schema  for Rent\nrent_schema = StructType([\n            StructField(\"Rent_id\", IntegerType(), False),\n            StructField(\"Rent_fee\", StringType(), True),\n            StructField(\"Late_fee\", StringType(), False),\n            StructField(\"Due_date\", TimestampType(), True),\n            StructField(\"Lease_id\", IntegerType(), True),\n            StructField(\"Pay_id\", IntegerType(), True)])\n\n# Schema for Payment\npayment_schema = StructType([\n            StructField(\"Payment_id\", IntegerType(), False),\n            StructField(\"Pay_date\", TimestampType(), True),\n            StructField(\"Pay_amount\", StringType(), False),\n            StructField(\"Method\", StringType(), True),\n            StructField(\"Rent_id\", IntegerType(), True)])\n\n# Schema for Apartment Maintenance\napt_maintenance_schema = StructType([\n            StructField(\"Maintenance_id\", IntegerType(), False),\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Mdate\", StringType(), False),\n            StructField(\"Issue_reported\", StringType(), True),\n            StructField(\"Contractor_id\", IntegerType(), True), \n            StructField(\"Resolution\", StringType(), True), \n            StructField(\"Status\", StringType(), True),\n            StructField(\"Charges_incurred\", StringType(), True)])\n\n# Schema for Building Maintenance\nbuilding_maintenance_schema = StructType([\n            StructField(\"Maintenance_id\", IntegerType(), False),\n            StructField(\"Building_name\", StringType(), True),\n            StructField(\"Ndate\", StringType(), False),\n            StructField(\"Issue_reported\", StringType(), True),\n            StructField(\"Contractor_id\", IntegerType(), True), \n            StructField(\"Resolution\", StringType(), True), \n            StructField(\"Status\", StringType(), True)])\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Function to get SparkDataFrame after reading JSON data from API\ndef getSparkDataFrame(url, schema):\n  appdf = requests.get(url)\n  objJSON = appdf.json()\n  a=[json.dumps(objJSON)]\n  jsonRDD = sc.parallelize(a)\n  df = spark.read.schema(schema).json(jsonRDD)\n  return df\n\n# convert string value to Float value\ndef string_to_float(x):\n  return float(x[1:])\n\n# Get DataFrame without new Line characters\n# Especially for Apartment, ApartmentMaintenance\n\ndef getSparkDataFrameWithoutLFChar(url, schema):\n  appdf = requests.get(url)\n  str=''\n  for line in appdf.iter_lines():\n    str = line.decode(encoding='utf-8', errors='strict')\n    # escaping \\n works for python3, if it's python 2 no need to escape\n    str = str.replace('\\\\n', '')\n  json_str=json.loads(str)\n  df = spark.createDataFrame(json_str, schema)\n  return df\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def unionAll(*dfs):\n  return reduce(DataFrame.unionAll, dfs)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def fixTenantRow(c):\n    # get the Mobile field\n    number = c.Mobile\n\n    # initialize variables \n    is_valid_number = \"N\"\n    clean_number = None\n    number_type = None\n    valid_mail = None\n\n    p = None\n\n    if number is not None:\n        # Clean the Mobile Number first\n        try:\n            p = phonenumbers.parse(number, None)\n\n            if phonenumbers.is_valid_number(p):\n                is_valid_number = \"Y\"\n            elif phonenumbers.truncate_too_long_number(p):\n                is_valid_number = \"Y\"\n            else:\n                is_valid_number = \"N\"\n\n            clean_number = \"%s%s\" % (p.country_code, p.national_number)\n            \n        except:\n            p = None\n\n    # clean up PhoneNumber\n    phone_no = c.Phone\n    if phone_no is not None:\n      phone_no = phone_no.replace(' ', '')\n      if (len(phone_no) != 10):\n        phone_no = None\n    \n    # validate Email \n    if re.match(r\"^[A-Za-z0-9\\.\\+_-]+@[A-Za-z0-9\\._-]+\\.[a-zA-Z]*$\", c.Email):\n      valid_mail = c.Email\n    \n    return Row( \n\t\tTenant_id = c.Tenant_id,\n\t\tFirst_name = c.First_name,\n\t\tLast_name = c.Last_name,\n\t\tSsn = c.Ssn,\n\t\tPhone = phone_no,\n\t\tEmail = valid_mail,\n        Mobile=clean_number \n    )"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# function validating Post Code  \ndef validatePostCode(postCode):\n  if (re.match(r\"^[0-9]{5}(-[0-9]{4})?$\", postCode)):\n    return postCode\n  else:\n    return None"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Create function to check each row if it exists in db\ndef checkIsRowExists(c, table_df):\n  \n  existing_row = table_df.filter(table_df.Landlord_id.isin(c.Landlord_id)) \n  if (existing_row.count() > 0):\n    return Row(\n        landlord_seq = existing_row.landlord_seq,\n        Landlord_id = c.Landlord_id,\n        Password = c.Password,\n        Landlord_name = c.Landlord_name,\n        Address_line_1 = c.Landlord_name,\n        City = c.City,\n        Post_code = c.Post_code,\n        Region = c.Region\n    )\n  else:\n    return Row(\n      landlord_seq = null,\n      Landlord_id = null,\n      Password = c.Password,\n      Landlord_name = c.Landlord_name,\n      Address_line_1 = c.Landlord_name,\n      City = c.City,\n      Post_code = c.Post_code,\n      Region = c.Region\n    )"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Functions to udf\nudfstring_to_float = udf(string_to_float, StringType())\n# UDF for validatePostCode function  \nudfValidatePostCode = udf(validatePostCode, StringType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["###### Description: Get landlord csv files as a streaming \"landlord_streamingDF\" and process it on the fly and get transformed stream \"landlord_df\"\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\""],"metadata":{}},{"cell_type":"code","source":["# Get Landlord Steaming DataFrame from csv files\n\n# streaming starts here by reading the input files \nlandlord_Path = \"/FileStore/apartment/landlord/inprogress/\"\nlandlord_streamingDF = (\n  spark\n    .readStream\n    .schema(landlord_schema)\n    .option(\"maxFilesPerTrigger\", \"1\")\n    .option(\"header\", \"true\")\n    .csv(landlord_Path)\n)\n\nlandlord_df = landlord_streamingDF.withColumn(\"Post_code\", udfValidatePostCode(\"Post_code\") )\n# landlord_df = landlord_df.select(landlord_df.Landlord_id, landlord_df.Password, landlord_df.Landlord_name, landlord_df.Address_line_1,  landlord_df.City, landlord_df.PostCode, landlord_df.Region, landlord_df.Seen)\n# Instantiation of DataTransformer class:\ntransformer = op.DataFrameTransformer(landlord_df)\n# Replace NA with 0's\ntransformer.replace_na(0.0, columns=\"*\")\n# Clear accents: clear_accents only from name column and not everywhere \ntransformer.clear_accents(columns='*')\n# Remove special characters:  From all Columns \ntransformer.remove_special_chars(columns=['Landlord_name', 'Address_line_1', 'City', 'Region'])\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["- ###### Now \"landlord_df\" contains pre-processed landlord state rows\n- ###### After this point we need comparison\n- ###### Stream-Stream subtraction is not supported\n- ###### So we dump the incoming data to a query result \"landlord_datalake\" which will give updated resulsts upon request\n- ###### \"landlord_datalake\" is not streaming but it will give updated results upon request\n- ###### From \"landlord_datalake\" we filter out the unseen rows to \"unseen_landlord_df\""],"metadata":{}},{"cell_type":"code","source":["landlord_datalake_query = landlord_df.writeStream.format(\"memory\").queryName(\"landlord_datalake\").start()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Enter batch mode"],"metadata":{}},{"cell_type":"markdown","source":["###### Take a snapshot of the landlord_df where Seen = false\n###### Add the fetch time columns to hive table landlord_seq_tracker"],"metadata":{}},{"cell_type":"code","source":["# OLD Code\n# def getDelta_df(entity):\n  \n#   #   Save snapshot of data into hive table to work with\n#   if (len(spark.sql(\"SHOW TABLES LIKE '\" + entity + \"_temp'\").collect()) == 1):\n#       spark.sql(\"drop table \" + entity + \"_temp\")\n#       spark.sql(\"select * from \" + entity + \"_datalake\").write.saveAsTable(entity + \"_temp\")\n#   else:\n#       spark.sql(\"select * from \" + entity + \"_datalake\").write.saveAsTable(entity + \"_temp\")\n      \n#   #   Take snapshot\n#   datalake_snapshot = spark.sql(\"select * from \" + entity + \"_temp\")\n    \n#   if (len(spark.sql(\"SHOW TABLES LIKE '\" + entity + \"_tracker'\").collect()) == 1):\n#     seq_tracker = spark.sql(\"select * from \" + entity + \"_tracker\")\n#     datalake_eq = (( datalake_snapshot\n#                     .join(seq_tracker, seq_tracker.sequence == datalake_snapshot.fetch_time))\n#                    .drop(\"sequence\").write.saveAsTable(\"temp_data\"))\n\n#     spark.sql(\"refresh table temp_data\")\n\n#     delta_df = datalake_snapshot.subtract(spark.sql(\"select * from temp_data\"))\n\n#     if (len(spark.sql(\"SHOW TABLES LIKE '\" + entity + \"_delta\" + \"'\").collect()) == 1):\n#       spark.sql(\"drop table \" + entity + \"_delta\")\n#       delta_df.write.saveAsTable(entity + \"_delta\")\n#     else:\n#       delta_df.write.saveAsTable(entity + \"_delta\")\n      \n#     delta_df.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.insertInto(entity + \"_tracker\")\n\n#     spark.sql(\"drop table temp_data\")\n#   else:\n#     datalake_snapshot.write.saveAsTable(entity + \"_delta\")\n#     datalake_snapshot.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.saveAsTable(entity + \"_tracker\")    \n  \n#   return spark.sql(\"select * from \" + entity + \"_delta\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def getDelta_df(entity):\n  \n  #   Save snapshot of data into hive table to work with\n  spark.sql(\"select * from \" + entity + \"_datalake\").write.mode(\"overwrite\").saveAsTable(entity + \"_temp\")\n  #   Take snapshot\n  datalake_snapshot = spark.sql(\"select * from \" + entity + \"_temp\")\n    \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n    seq_tracker = spark.sql(\"select * from \" + entity + \"_tracker\")\n    datalake_eq = (( datalake_snapshot\n                    .join(seq_tracker, seq_tracker.sequence == datalake_snapshot.fetch_time))\n                   .drop(\"sequence\").write.saveAsTable(\"temp_data\"))\n\n    spark.sql(\"refresh table temp_data\")\n\n    delta_df = datalake_snapshot.subtract(spark.sql(\"select * from temp_data\"))\n    \n    delta_df.write.mode(\"overwrite\").saveAsTable(entity + \"_delta\")\n      \n    delta_df.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.insertInto(entity + \"_tracker\")\n\n    spark.sql(\"drop table temp_data\")\n  else:\n    datalake_snapshot.write.saveAsTable(entity + \"_delta\")\n    datalake_snapshot.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.saveAsTable(entity + \"_tracker\")    \n  \n  return spark.sql(\"select * from \" + entity + \"_delta\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["def resetTrackingData(entity):\n  if (len(spark.sql(\"show tables like '\" + entity + \"_delta'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_delta\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_temp'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_temp\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_tracker\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["resetTrackingData(\"landlord\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["spark.sql(\"drop table landlord_data\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["def getLastLandlordState_df():\n  entity = \"landlord\"\n  delta_df = getDelta_df(entity).drop(\"fetch_time\")\n  temp_state_df = ( delta_df.groupBy(\"Landlord_id\").agg(F.max(delta_df.event_time))\n                   .select(col(\"Landlord_id\").alias(\"Landlord_id1\"), col(\"max(event_time)\").alias(\"event_time1\")))\n  delta_state_df = ( delta_df.join(temp_state_df,(delta_df.Landlord_id == temp_state_df.Landlord_id1) \n                                                & (delta_df.event_time == temp_state_df.event_time1))\n                    .drop(\"Landlord_id1\")\n                    .drop(\"event_time1\"))\n  \n  return delta_state_df"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["def replaceLandlordRows(update_df):\n  for row in update_df.collect():\n    spark.sql(\"delete from landlord_data where Landlord_id=\" + str(row.Landlord_id))\n  update_df.write.mode(\"append\").saveAsTable(\"landlord_data\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["def updateLandlord(new_State_df):\n  \n  spark.sql(\"create table if not exists landlord_data (Landlord_id int, Password string, Landlord_name string, Address_line_1 string, City string, Post_code string, Region string, event_time timestamp)\")\n  \n  landlord_data_df = (spark.sql(\"select * from landlord_data\")\n                      .select(col(\"Landlord_id\").alias(\"Landlord_id1\"), \n                              col(\"Password\").alias(\"Password1\"), \n                              col(\"Landlord_name\").alias(\"Landlord_name1\"), \n                              col(\"Address_line_1\").alias(\"Address_line_11\"), \n                              col(\"City\").alias(\"City1\"), \n                              col(\"Post_code\").alias(\"Post_code1\"), \n                              col(\"Region\").alias(\"Region1\"), \n                              col(\"event_time\").alias(\"event_time1\")))\n  \n  update_rows_df = (landlord_data_df.join(state_df, (new_State_df.Landlord_id == landlord_data_df.Landlord_id1),  'outer')\n             .select(new_State_df.Landlord_id, \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.Password)\n                     .otherwise(landlord_data_df.Password1).alias(\"Password\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.Landlord_name)\n                     .otherwise(landlord_data_df.Landlord_name1).alias(\"Landlord_name\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.Address_line_1)\n                     .otherwise(landlord_data_df.Address_line_11).alias(\"Address_line_1\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.City)\n                     .otherwise(landlord_data_df.City1).alias(\"City\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.Post_code)\n                     .otherwise(landlord_data_df.Post_code1).alias(\"Post_code\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.Region)\n                     .otherwise(landlord_data_df.Region1).alias(\"Region\"), \n                     \n                     F.when(new_State_df.event_time > landlord_data_df.event_time1, new_State_df.event_time)\n                     .otherwise(landlord_data_df.event_time1).alias(\"event_time\")))\n  \n  new_ids_df = (new_State_df.select(\"Landlord_id\").subtract(update_rows_df.select(\"Landlord_id\"))\n                .distinct().select(col(\"Landlord_id\").alias(\"Landlord_id1\")))\n  \n  new_rows_df = (new_State_df.join(new_ids_df, (new_State_df.Landlord_id == new_ids_df.Landlord_id1), \"outer\")\n                 .drop(\"Landlord_id1\")\n                 .distinct())\n  \n  new_rows_df.write.insertInto(\"landlord_data\")\n  \n#   replaceLandlordRows(update_rows_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["state_df = getLastLandlordState_df()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(state_df.orderBy(\"Landlord_id\"))"],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"Process_Landlord","notebookId":469737316778599},"nbformat":4,"nbformat_minor":0}
