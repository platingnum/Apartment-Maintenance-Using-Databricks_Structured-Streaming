{"cells":[{"cell_type":"markdown","source":["###### Description: In this notebook we read landlord state rows from incoming csv files into a streamig dataframe, transform (clean, cast, rename) the data, add/update the latest state to a static hive tabe\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\" --> \"landlord_data\""],"metadata":{}},{"cell_type":"code","source":["import requests\nimport json\nimport optimus as op\nimport phonenumbers \nimport re\nimport datetime\n\nfrom pyspark.sql.types import StringType, IntegerType, TimestampType, DateType, DoubleType, StructType, StructField\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window as W\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import rank, col\nimport time"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Schema for building JSON\nbuilding_schema = StructType([\n            StructField(\"Building_id\", IntegerType(), False),\n            StructField(\"Building_name\", StringType(), True),\n            StructField(\"Landlord_id\", IntegerType(), False),\n            StructField(\"Address_line_1\", StringType(), False),\n            StructField(\"City\", StringType(), False),\n            StructField(\"Post_code\", StringType(), True),\n            StructField(\"Region\", StringType(), True),\n            StructField(\"event_time\", TimestampType(), True),\n            StructField(\"fetch_time\", StringType(), True)])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# function validating Post Code  \ndef validatePostCode(postCode):\n  if (re.match(r\"^[0-9]{5}(-[0-9]{4})?$\", postCode)):\n    return postCode\n  else:\n    return \"\"\n  \n# UDF for validatePostCode function  \nudfValidatePostCode = udf(validatePostCode, StringType())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["###### Description: Get landlord csv files as a streaming \"landlord_streamingDF\" and process it on the fly and get transformed stream \"landlord_df\"\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\""],"metadata":{}},{"cell_type":"code","source":["# Get Building Steaming DataFrame from csv files\n\n# streaming starts here by reading the input files \nbuilding_Path = \"/FileStore/apartment/building/inprogress/\"\nbuilding_streamingDF = (\n  spark\n    .readStream\n    .schema(building_schema)\n    .option(\"maxFilesPerTrigger\", \"1\")\n    .option(\"header\", \"true\")\n    .csv(building_Path)\n)\n\n# building_streamingDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/apartment/building/inprogress/part-00000-tid-7368714421418765704-83751609-b4eb-4e45-9ebb-c4a51c35008f-21-c000.csv\")\n\nbuilding_df = building_streamingDF.withColumn(\"Post_code\", udfValidatePostCode(\"Post_code\") )\n# Instantiation of DataTransformer class:\ntransformer = op.DataFrameTransformer(building_df)\n# Replace NA with 0's\ntransformer.replace_na(0.0, columns=\"*\")\n# Clear accents: clear_accents only from name column and not everywhere \ntransformer.clear_accents(columns='*')\n# Remove special characters:  From all Columns \ntransformer.remove_special_chars(columns=['Building_name', 'Address_line_1', 'City', 'Region'])\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(building_df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["- ###### Now \"landlord_df\" contains pre-processed landlord state rows\n- ###### After this point we need comparison\n- ###### Stream-Stream subtraction is not supported\n- ###### So we dump the incoming data to a query result \"landlord_datalake\" which will give updated resulsts upon request\n- ###### \"landlord_datalake\" is not streaming but it will give updated results upon request\n- ###### From \"landlord_datalake\" we filter out the unseen rows to \"unseen_landlord_df\""],"metadata":{}},{"cell_type":"code","source":["building_datalake_query = building_df.writeStream.format(\"memory\").queryName(\"building_datalake\").start()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Enter batch mode"],"metadata":{}},{"cell_type":"markdown","source":["###### Take a snapshot of the landlord_df where Seen = false\n###### Add the fetch time columns to hive table landlord_seq_tracker"],"metadata":{}},{"cell_type":"code","source":["def getDelta_df(entity):\n  \n  #   Save snapshot of data into hive table to work with\n  spark.sql(\"select * from \" + entity + \"_datalake\").write.mode(\"overwrite\").saveAsTable(entity + \"_temp\")\n  #   Take snapshot\n  datalake_snapshot = spark.sql(\"select * from \" + entity + \"_temp\")\n    \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n    seq_tracker = spark.sql(\"select * from \" + entity + \"_tracker\")\n    datalake_eq = (( datalake_snapshot\n                    .join(seq_tracker, seq_tracker.sequence == datalake_snapshot.fetch_time))\n                   .drop(\"sequence\").write.saveAsTable(\"temp_data\"))\n\n    spark.sql(\"refresh table temp_data\")\n\n    delta_df = datalake_snapshot.subtract(spark.sql(\"select * from temp_data\"))\n    \n    delta_df.write.mode(\"overwrite\").saveAsTable(entity + \"_delta\")\n      \n    delta_df.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.insertInto(entity + \"_tracker\")\n\n    spark.sql(\"drop table temp_data\")\n  else:\n    datalake_snapshot.write.saveAsTable(entity + \"_delta\")\n    datalake_snapshot.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.saveAsTable(entity + \"_tracker\")    \n  \n  return spark.sql(\"select * from \" + entity + \"_delta\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def resetTrackingData(entity):\n  if (len(spark.sql(\"show tables like '\" + entity + \"_delta'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_delta\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_temp'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_temp\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_tracker\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["resetTrackingData(\"building\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["spark.sql(\"drop table building_data\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["def getLastBuildingState_df():\n  entity = \"building\"\n  delta_df = getDelta_df(entity).drop(\"fetch_time\")\n  temp_state_df = ( delta_df.groupBy(\"Building_id\").agg(F.max(delta_df.event_time))\n                   .select(col(\"Building_id\").alias(\"Building_id1\"), col(\"max(event_time)\").alias(\"event_time1\")))\n  delta_state_df = ( delta_df.join(temp_state_df,(delta_df.Building_id == temp_state_df.Building_id1) \n                                                & (delta_df.event_time == temp_state_df.event_time1))\n                    .drop(\"Building_id1\")\n                    .drop(\"event_time1\"))\n  \n  return delta_state_df"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def replaceBuildingRows(update_df):\n  for row in update_df.collect():\n    spark.sql(\"delete from landlord_data where Building_id=\" + str(row.Building_id))\n  update_df.write.mode(\"append\").saveAsTable(\"building_data\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["building_df"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def updateBuilding(new_State_df):\n  \n  spark.sql(\"create table if not exists building_data (Building_id int, Building_name string, Landlord_id int, Address_line_1 string, City string, Post_code string, Region string, event_time timestamp)\")\n  \n  building_data_df = (spark.sql(\"select * from building_data\")\n                      .select(col(\"Building_id\").alias(\"Building_id1\"), \n                              col(\"Building_name\").alias(\"Building_name1\"), \n                              col(\"Landlord_id\").alias(\"Landlord_id1\"), \n                              col(\"Address_line_1\").alias(\"Address_line_11\"), \n                              col(\"City\").alias(\"City1\"), \n                              col(\"Post_code\").alias(\"Post_code1\"), \n                              col(\"Region\").alias(\"Region1\"), \n                              col(\"event_time\").alias(\"event_time1\")))\n  \n  update_rows_df = (building_data_df.join(state_df, (new_State_df.Building_id == building_data_df.Building_id1),  'outer')\n             .select(new_State_df.Building_id, \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.Building_name)\n                     .otherwise(building_data_df.Building_name1).alias(\"Password\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.Landlord_id)\n                     .otherwise(building_data_df.Landlord_id1).alias(\"Landlord_name\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.Address_line_1)\n                     .otherwise(building_data_df.Address_line_11).alias(\"Address_line_1\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.City)\n                     .otherwise(building_data_df.City1).alias(\"City\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.Post_code)\n                     .otherwise(building_data_df.Post_code1).alias(\"Post_code\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.Region)\n                     .otherwise(building_data_df.Region1).alias(\"Region\"), \n                     \n                     F.when(new_State_df.event_time > building_data_df.event_time1, new_State_df.event_time)\n                     .otherwise(building_data_df.event_time1).alias(\"event_time\")))\n  \n  new_ids_df = (new_State_df.select(\"Building_id\").subtract(update_rows_df.select(\"Building_id\"))\n                .distinct().select(col(\"Building_id\").alias(\"Building_id1\")))\n  \n  new_rows_df = (new_State_df.join(new_ids_df, (new_State_df.Building_id == new_ids_df.Building_id1), \"outer\")\n                 .drop(\"Building_id1\")\n                 .distinct())\n  \n  new_rows_df.write.insertInto(\"building_data\")\n  \n#   replaceBuildingRows(update_rows_df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["state_df = getLastBuildingState_df()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(state_df.orderBy(\"Building_id\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["updateBuilding(state_df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(spark.sql(\"select * from building_data\").orderBy(\"Building_id\"))"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Process_Building","notebookId":469737316778637},"nbformat":4,"nbformat_minor":0}
