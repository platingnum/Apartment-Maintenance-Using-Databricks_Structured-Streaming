{"cells":[{"cell_type":"markdown","source":["###### Description: In this notebook we read landlord state rows from incoming csv files into a streamig dataframe, transform (clean, cast, rename) the data, add/update the latest state to a static hive tabe\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\" --> \"landlord_data\""],"metadata":{}},{"cell_type":"code","source":["import requests\nimport json\nimport optimus as op\nimport phonenumbers \nimport re\nimport datetime\nimport pandas as pd\n\nfrom pyspark.sql.types import StringType, IntegerType, TimestampType, DateType, DoubleType, StructType, StructField\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window as W\nfrom functools import reduce  # For Python 3.x\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import rank, col\nimport time"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Schema for Apartment JSON\napartment_schema = StructType([\n            StructField(\"Building_id\", IntegerType(), True),\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Type\", StringType(), True),\n            StructField(\"Rent_fee\", StringType(), True),\n            StructField(\"Building_name\", StringType(), True),\n            StructField(\"Appt_details\", StringType(), True),\n            StructField(\"event_time\", TimestampType(), True),\n            StructField(\"fetch_time\", StringType(), True)])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###### Description: Get landlord csv files as a streaming \"landlord_streamingDF\" and process it on the fly and get transformed stream \"landlord_df\"\n###### Objective: (incoming csv files) --> \"landlord_streamingDF\" --> \"landlord_df\""],"metadata":{}},{"cell_type":"code","source":["# Get Landlord Steaming DataFrame from csv files\n\n# streaming starts here by reading the input files \napartment_Path = \"/FileStore/apartment/apartment/inprogress/\"\napartment_streamingDF = (\n  spark\n    .readStream\n    .schema(apartment_schema)\n    .option(\"maxFilesPerTrigger\", \"1\")\n    .option(\"header\", \"true\")\n    .option(\"multiLine\", \"true\")\n    .csv(apartment_Path)\n)\n\napartment_df = apartment_streamingDF.select(\"*\").where(\"Building_id IS NOT NULL\")\n# landlord_df = landlord_df.select(landlord_df.Landlord_id, landlord_df.Password, landlord_df.Landlord_name, landlord_df.Address_line_1,  landlord_df.City, landlord_df.PostCode, landlord_df.Region, landlord_df.Seen)\n# Instantiation of DataTransformer class:\ntransformer = op.DataFrameTransformer(apartment_df)\n# Replace NA with 0's\ntransformer.replace_na(0.0, columns=\"*\")\n# Clear accents: clear_accents only from name column and not everywhere \ntransformer.clear_accents(columns='*')\n# Remove special characters:  From all Columns \ntransformer.remove_special_chars(columns=['Type', 'Rent_fee', 'Building_name', 'Appt_details'])\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["- ###### Now \"landlord_df\" contains pre-processed landlord state rows\n- ###### After this point we need comparison\n- ###### Stream-Stream subtraction is not supported\n- ###### So we dump the incoming data to a query result \"landlord_datalake\" which will give updated resulsts upon request\n- ###### \"landlord_datalake\" is not streaming but it will give updated results upon request\n- ###### From \"landlord_datalake\" we filter out the unseen rows to \"unseen_landlord_df\""],"metadata":{}},{"cell_type":"code","source":["apartment_datalake_query = apartment_df.writeStream.format(\"memory\").queryName(\"apartment_datalake\").start()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Enter batch mode"],"metadata":{}},{"cell_type":"markdown","source":["###### Take a snapshot of the landlord_df where Seen = false\n###### Add the fetch time columns to hive table landlord_seq_tracker"],"metadata":{}},{"cell_type":"code","source":["def getDelta_df(entity):\n  \n  #   Save snapshot of data into hive table to work with\n  spark.sql(\"select * from \" + entity + \"_datalake\").write.mode(\"overwrite\").saveAsTable(entity + \"_temp\")\n  #   Take snapshot\n  datalake_snapshot = spark.sql(\"select * from \" + entity + \"_temp\")\n    \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n    seq_tracker = spark.sql(\"select * from \" + entity + \"_tracker\")\n    datalake_eq = (( datalake_snapshot\n                    .join(seq_tracker, seq_tracker.sequence == datalake_snapshot.fetch_time))\n                   .drop(\"sequence\").write.saveAsTable(\"temp_data\"))\n\n    spark.sql(\"refresh table temp_data\")\n\n    delta_df = datalake_snapshot.subtract(spark.sql(\"select * from temp_data\"))\n    \n    delta_df.write.mode(\"overwrite\").saveAsTable(entity + \"_delta\")\n      \n    delta_df.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.insertInto(entity + \"_tracker\")\n\n    spark.sql(\"drop table temp_data\")\n  else:\n    datalake_snapshot.write.saveAsTable(entity + \"_delta\")\n    datalake_snapshot.select(col(\"fetch_time\").alias(\"sequence\")).distinct().write.saveAsTable(entity + \"_tracker\")    \n  \n  return spark.sql(\"select * from \" + entity + \"_delta\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["def resetTrackingData(entity):\n  if (len(spark.sql(\"show tables like '\" + entity + \"_delta'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_delta\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_temp'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_temp\")\n      \n  if (len(spark.sql(\"show tables like '\" + entity + \"_tracker'\").collect()) == 1):\n      spark.sql(\"drop table \" + entity + \"_tracker\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def getLastApartmentState_df():\n  entity = \"apartment\"\n  delta_df = getDelta_df(entity).drop(\"fetch_time\")\n  temp_state_df = ( delta_df.groupBy(\"Building_id\", \"Apartment_number\").agg(F.max(delta_df.event_time))\n                   .select(col(\"Building_id\").alias(\"Building_id1\"), \n                           col(\"Apartment_number\").alias(\"Apartment_number1\"), \n                           col(\"max(event_time)\").alias(\"event_time1\")))\n  delta_state_df = ( delta_df.join(temp_state_df,(delta_df.Building_id == temp_state_df.Building_id1)\n                                           &(delta_df.Apartment_number == temp_state_df.Apartment_number1) \n                                                & (delta_df.event_time == temp_state_df.event_time1))\n                    .drop(\"Building_id1\")\n                    .drop(\"Apartment_number1\")\n                    .drop(\"event_time1\"))\n  \n  return delta_state_df"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def updateApartment(new_State_df):\n  \n  if (len(spark.sql(\"select * from delta.`/delta/apartment/apartment_data`\").collect()) == 0):\n      new_State_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/apartment/apartment_data\")\n  else:\n    new_state_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/apartment/apartment_data_temp\")\n    \n    query_str = \"MERGE INTO delta.`/delta/apartment/apartment_data` AS apartment_data \\\n    USING delta.`/delta/apartment/apartment_data_temp` AS apartment_data_temp \\\n    ON apartment_data.Building_id = apartment_data_temp.Building_id \\\n    AND apartment_data.Apartment_number = apartment_data_temp.Apartment_number \\\n    WHEN MATCHED THEN \\\n      UPDATE SET \\\n        apartment_data.Type = apartment_data_temp.Type, \\\n        apartment_data.Rent_fee = apartment_data_temp.Rent_fee, \\\n        apartment_data.Building_name = apartment_data_temp.Building_name, \\\n        apartment_data.Appt_details = apartment_data_temp.Appt_details, \\\n        apartment_data.event_time = apartment_data_temp.event_time \\\n    WHEN NOT MATCHED \\\n      THEN INSERT (Building_id, \\\n      Apartment_number, Type, Rent_fee, Building_name, Appt_details, event_time) VALUES (apartment_data_temp.Building_id, \\\n      apartment_data_temp.Apartment_number, apartment_data_temp.Type, \\\n      apartment_data_temp.Rent_fee, apartment_data_temp.Building_name, apartment_data_temp.Appt_details, apartment_data_temp.event_time)\"\n    \n    spark.sql(query_str)\n  return spark.sql(\"select * from delta.`/delta/apartment/apartment_data`\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Delta Table operation execution"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"select * from apartment_datalake where false\").drop(\"fetch_time\").write.format(\"delta\").mode(\"overwrite\").save(\"/delta/apartment/apartment_data\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(apartment_df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(spark.sql(\"select * from apartment_datalake\").orderBy(\"Building_id\", \"Apartment_number\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["new_state_df = getLastApartmentState_df()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["new_state_df.count()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["apartment_current_data_df = updateApartment(new_state_df)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(apartment_current_data_df.orderBy(\"Building_id\", \"Apartment_number\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["apartment_current_data_df.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["resetTrackingData(\"apartment\")\nspark.sql(\"delete from delta.`/delta/apartment/apartment_data`\")"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Process_Apartment","notebookId":469737316778884},"nbformat":4,"nbformat_minor":0}
